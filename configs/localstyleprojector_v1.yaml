model:
  target: visconet.control_cond_modules.module_main.LocalStyleProjector
  params:
    num_fashion_attrs: 8
    uncond_guidance: True
    # NOTE: To get fashion attributes from source image by segmenting
    fashion_segmentor_config:
      target: visconet.control_cond_modules.module_segmentor.FashionSegmentor
      params:
        seg_processor: "sayeed99/segformer_b3_clothes"
        seg_model: "sayeed99/segformer_b3_clothes"
        output_shape: [224, 224]
        ignore_labels:
          - "Belt"
          - "Scarf"
          - "Bag"
          - "Left-leg"
          - "Right-leg"
          - "Left-arm"
          - "Right-arm"
    # NOTE:  To get image embeddings from fashion attributes
    # for CLIP image encoder
    image_encoder_config:
      target: visconet.control_cond_modules.module_img_embeddor.CLIPImageEncoder
      params:
        encoder_type: "CLIP"
        encoder_processor_name: "openai/clip-vit-large-patch14"
        encoder_model_name: "openai/clip-vit-large-patch14"
    # for DINO image encoder
    # image_encoder_config:
    #   target: visconet.control_cond_modules.module_img_embeddor.DINOImageEncoder
    #   params:
    #     encoder_type: "DINO"
    #     encoder_processor_name: "facebook/dinov2-base"
    #     encoder_model_name: "facebook/dinov2-base"
    # NOTE: To resample image embeddings to get richer embeddings
    resampler_config:
      target: adapters.resampler.Resampler
      params:
        dim: 1024
        depth: 8
        dim_head: 64
        heads: 16
        num_queries: 4 # TODO: Currently fixed to 4 queries per attribute since we have 8 attributes
        embedding_dim: 1024 # NOTE: Use 1024 with CLIP else 768 with DINO
        output_dim: 1024
        ff_mult: 4
        max_input_seq_len: 257
        apply_pos_emb: True
