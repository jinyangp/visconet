model:
  target: visconet.visconet.ViscoNetLDM
  params:
    linear_start: 0.00085
    linear_end: 0.0120
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: "jpg"
    cond_stage_key: "txt"
    control_key: "hint"
    control_crossattn_key: "styles"
    mask_key: "human_mask"
    image_size: 64
    channels: 4
    cond_stage_trainable: false
    conditioning_key: crossattn
    monitor: val/loss_simple_ema
    scale_factor: 0.18215
    use_ema: False
    only_mid_control: False

    scheduler_config:
      target: torch.optim.lr_scheduler.ReduceLROnPlateau
      monitor: val/loss_simple_ema
      params:
        mode: min
        factor: 0.5
        patience: 3
        cooldown: 0
        min_lr: 0.00001
        threshold: 0.001
        verbose: True

    control_cond_config:
      target: visconet.control_cond_modules.module_main.LocalStyleProjector
      params:
        num_fashion_attrs: 5
        uncond_guidance: True
        output_height: 512 # to match output dimensions of visconet
        output_width: 512
        use_baseline: True

        # NOTE: To segment background and foreground first
        human_segmentor_config:
          target: visconet.control_cond_modules.module_human_segmentor.HumanSegmentor
          params:
            model_name: "resnet_101"
            image_height: 768 # NOTE: found that larger images tend to be segmented by FashionSegmentor
            image_width: 768 # NOTE: found that larger images tend to be segmented by FashionSegmentor
            num_classes: 21

        # NOTE: To get fashion attributes from source image by segmenting
        fashion_segmentor_config:
          target: visconet.control_cond_modules.module_fashion_segmentor.FashionSegmentor
          params:
            seg_processor: "mattmdjaga/segformer_b2_clothes" # "sayeed99/segformer_b3_clothes"
            seg_model: "mattmdjaga/segformer_b2_clothes" # "sayeed99/segformer_b3_clothes"
            valid_threshold: 0.002
            output_shape: [224, 224]
            ignore_labels:
              - "Belt"
              - "Scarf"
              - "Bag"
              - "Left-leg"
              - "Right-leg"
              - "Left-arm"
              - "Right-arm"
              - "Background"
              - "Sunglasses"
              - "Hat"
            target_labels:
              - "Face"
              - "Hair"
              - "Pants"
              - "Upper-clothes"
              - "Left-shoe"
              - "Right-shoe"
            default_seg_map_id2labels:
              1: "top"
              2: "outer"
              3: "skirt"
              4: "dress"
              7: "headwear"
              21: "rompers"
              5: "pants"
              11: "footwear"
              13: "hair"
              14: "face"

        # NOTE:  To get image embeddings from fashion attributes
        # for CLIP image encoder
        image_encoder_config:
          target: visconet.control_cond_modules.module_img_embeddor.CLIPImageEncoder
          params:
            encoder_type: "CLIP"
            encoder_processor_name: "openai/clip-vit-large-patch14"
            encoder_model_name: "openai/clip-vit-large-patch14"
        # for DINO image encoder
        # image_encoder_config:
        #   target: visconet.control_cond_modules.module_img_embeddor.DINOImageEncoder
        #   params:
        #     encoder_type: "DINO"
        #     encoder_processor_name: "facebook/dinov2-base"
        #     encoder_model_name: "facebook/dinov2-base"

        # NOTE: To resample image embeddings to get richer embeddings
        resampler_config:
          target: visconet.modules.ProjectLocalStyle
          params:
            pool_size: 9
            local_emb_size: 257
            bias: True
            #target: visconet.modules.ClipImageEncoder

    control_stage_config:
      target: cldm.cldm.ControlNet
      params:
        use_checkpoint: True
        image_size: 32 # unused
        in_channels: 4
        hint_channels: 3
        model_channels: 320
        attention_resolutions: [4, 2, 1]
        num_res_blocks: 2
        channel_mult: [1, 2, 4, 4]
        num_head_channels: 64 # need to fix for flash-attn
        use_spatial_transformer: True
        use_linear_in_transformer: True
        transformer_depth: 1
        context_dim: 1024
        legacy: False

    unet_config:
      target: cldm.cldm.ControlledUnetModel
      params:
        use_checkpoint: True
        image_size: 32 # unused
        in_channels: 4
        out_channels: 4
        model_channels: 320
        attention_resolutions: [4, 2, 1]
        num_res_blocks: 2
        channel_mult: [1, 2, 4, 4]
        num_head_channels: 64 # need to fix for flash-attn
        use_spatial_transformer: True
        use_linear_in_transformer: True
        transformer_depth: 1
        context_dim: 1024
        legacy: False

    first_stage_config:
      target: ldm.models.autoencoder.AutoencoderKL
      params:
        embed_dim: 4
        monitor: val/rec_loss
        ddconfig:
          #attn_type: "vanilla-xformers"
          double_z: true
          z_channels: 4
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
            - 1
            - 2
            - 4
            - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity

    cond_stage_config:
      target: ldm.modules.encoders.modules.FrozenOpenCLIPEmbedder
      params:
        freeze: True
        layer: "penultimate"

  style_embedding_config:
    target: scripts.image_emb_hidden.ClipImageEncoder

dataset:
  train:
    target: visconet.deepfashion.DeepFashionDataset
    params:
      image_root: "./data/datasets/deepfashion"
      image_dir: imgs
      style_dir: seg_imgs
      map_file: "./data/deepfashion/deepfashion_multimodal_map_sample.csv"
      data_files:
        - "./data/deepfashion/pairs-train-all_multimodal_sample.csv" # to and from are different

  val:
    target: visconet.deepfashion.DeepFashionDataset
    params:
      image_root: "./data/datasets/deepfashion"
      image_dir: imgs
      style_dir: seg_imgs
      map_file: "./data/deepfashion/deepfashion_multimodal_map_sample.csv"
      data_files:
        - "./data/deepfashion/pairs-test-all_multimodal_sample.csv" # to and from are different

  test:
    target: visconet.deepfashion.DeepFashionDataset
    params:
      image_root: "./data/datasets/deepfashion"
      image_dir: imgs
      style_dir: seg_imgs
      map_file: "./data/deepfashion/deepfashion_multimodal_map_sample.csv"
      data_files:
        - "./data/deepfashion/pairs-test-all_multimodal_sample.csv" # to and from are different
